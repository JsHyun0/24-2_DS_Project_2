{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss 정상화를 해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from Models.model import BaseModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from utils.utils import *\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection\n",
    "cat_features = ['Gender', 'Card Brand', 'Card Type', 'Expires', 'Has Chip', 'Year PIN last Changed', 'Whether Security Chip is Used', 'Day', 'Error Message']\n",
    "\n",
    "num_features = ['Current Age', 'Retirement Age', 'Per Capita Income - Zipcode', 'Yearly Income', 'Total Debt', 'Credit Score', 'Credit Limit', 'Amount','Since Open Month']\n",
    "\n",
    "discarded = ['User', 'Birth Year', 'Birth Month', 'Card', 'Card Number', 'Zipcode', 'Merchandise Code', 'Acct Open Date', 'Year', 'Month']\n",
    "\n",
    "print(len(cat_features)*5 + len(num_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data_path, cat_features, num_features, mode = 'AE'):\n",
    "    df = pd.read_csv(data_path)\n",
    "    ## 파생 변 수 생성\n",
    "    df['Error Message'] = df['Error Message'].astype(bool)\n",
    "    df['Since Open Month'] = (df['Year'] - df['Acct Open Date'].str[-4:].astype(int)) * 12 + (df['Month'] - df['Acct Open Date'].str[:2].astype(int)).astype(int)\n",
    "\n",
    "    ## IQR을 사용한 이상치 제거\n",
    "    for col in num_features:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR  \n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "    ## train-valid 분리\n",
    "    mask = df['Month'].between(1, 9)\n",
    "    train_df = df[mask]\n",
    "    valid_df = df[~mask]\n",
    "    if mode == 'AE':\n",
    "        train_df = train_df[train_df['Is Fraud?'] == 'No']    \n",
    "\n",
    "    ## 정규화\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_df[num_features])\n",
    "\n",
    "    train_df[num_features] = pd.DataFrame(\n",
    "        scaler.transform(train_df[num_features]),\n",
    "        columns = num_features,\n",
    "        index = train_df.index,\n",
    "    )\n",
    "    valid_df[num_features] = pd.DataFrame(\n",
    "        scaler.transform(valid_df[num_features]),\n",
    "        columns = num_features,\n",
    "        index = valid_df.index,\n",
    "    )\n",
    "\n",
    "    ## Label Encoding\n",
    "    label_encoders = {}\n",
    "    for col in cat_features:\n",
    "        le = LabelEncoder()\n",
    "        train_df[col] = le.fit_transform(train_df[col])\n",
    "        valid_df[col] = le.transform(valid_df[col])\n",
    "        label_encoders[col] = le\n",
    "        \n",
    "    ## 최종 데이터 분리\n",
    "    train_cat_X = train_df[cat_features]\n",
    "    train_num_X = train_df[num_features]\n",
    "    train_y = train_df['Is Fraud?'].astype(int)\n",
    "    \n",
    "    valid_cat_X = valid_df[cat_features]\n",
    "    valid_num_X = valid_df[num_features]\n",
    "    valid_y = valid_df['Is Fraud?'].astype(int)\n",
    "    return (train_cat_X, train_num_X, train_y), (valid_cat_X, valid_num_X, valid_y), label_encoders\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "data_path = 'Data/[24-2 DS_Project2] Data.csv'\n",
    "(train_cat_X, train_num_X, train_y), (valid_cat_X, valid_num_X, valid_y), label_encoders = process_data(\n",
    "    data_path,\n",
    "    cat_features,\n",
    "    num_features\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model, Dataset, DataLoader 설정\n",
    "## 3. 학습 및 검증\n",
    "## 4. 결과 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(BaseModel):\n",
    "    def __init__(self, encoding_dim, cat_features, num_features, num_classes=1):\n",
    "        super(AutoEncoder, self).__init__(encoding_dim, cat_features, num_features, num_classes)\n",
    "        self.input_dim = len(cat_features) * 5 + len(num_features)\n",
    "        \n",
    "        # 더 깊은 디코더 네트워크 구성\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),  # 드롭아웃 추가\n",
    "            nn.Linear(64, 48),\n",
    "            nn.BatchNorm1d(48),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),  # 드롭아웃 추가\n",
    "            nn.Linear(48, self.input_dim)\n",
    "        )\n",
    "        \n",
    "        # 임베딩 레이어 초기화 수정\n",
    "        for emb in self.cat_embeddings:\n",
    "            nn.init.uniform_(emb.weight, -0.05, 0.05)\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        embeddings = [emb(x_cat[:, i]) for i, emb in enumerate(self.cat_embeddings)]\n",
    "        # 임베딩 벡터 정규화 추가\n",
    "        normalized_embeddings = [torch.nn.functional.normalize(emb, p=2, dim=1) for emb in embeddings]\n",
    "        original_x = torch.cat(normalized_embeddings + [x_num], dim=1)\n",
    "        x = self.fc_cat(original_x)\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, original_x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
